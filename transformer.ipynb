{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1457,"sourceType":"datasetVersion","datasetId":781}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:47:48.010031Z","iopub.execute_input":"2024-04-07T23:47:48.010453Z","iopub.status.idle":"2024-04-07T23:47:55.906879Z","shell.execute_reply.started":"2024-04-07T23:47:48.010425Z","shell.execute_reply":"2024-04-07T23:47:55.906051Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:47:55.909139Z","iopub.execute_input":"2024-04-07T23:47:55.910020Z","iopub.status.idle":"2024-04-07T23:48:04.457822Z","shell.execute_reply.started":"2024-04-07T23:47:55.909985Z","shell.execute_reply":"2024-04-07T23:48:04.456732Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc1897a95a54f589fec6a5508277a4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96bf65430ed4e02b131aadca2732280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d03f557351d49c3bdd0f9623c2fa036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1968408f2ca44dc28d9130dbaa6c5376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3ed68fe0184bdfbb1c6551da3d4a1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"826e48e72b7c4c9d8a84029b2dd3c553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa564342f35415694cf472495eeab7c"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass JokesDataset(Dataset):\n    def __init__(self, jokes_dataset_path = '/kaggle/input/short-jokes'):\n        super().__init__()\n\n        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n\n        self.joke_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n        \n        with open(short_jokes_path) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n            \n            x = 0\n            for row in csv_reader:\n                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n                self.joke_list.append(joke_str)\n        \n    def __len__(self):\n        return len(self.joke_list)\n\n    def __getitem__(self, item):\n        return self.joke_list[item]","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:48:04.459233Z","iopub.execute_input":"2024-04-07T23:48:04.459552Z","iopub.status.idle":"2024-04-07T23:48:04.467317Z","shell.execute_reply.started":"2024-04-07T23:48:04.459528Z","shell.execute_reply":"2024-04-07T23:48:04.466388Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = JokesDataset()\njoke_loader = DataLoader(dataset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:48:04.468496Z","iopub.execute_input":"2024-04-07T23:48:04.468743Z","iopub.status.idle":"2024-04-07T23:48:05.027195Z","shell.execute_reply.started":"2024-04-07T23:48:04.468722Z","shell.execute_reply":"2024-04-07T23:48:05.026287Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 5000\nMAX_SEQ_LEN = 400\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:48:05.029188Z","iopub.execute_input":"2024-04-07T23:48:05.029487Z","iopub.status.idle":"2024-04-07T23:48:05.045867Z","shell.execute_reply.started":"2024-04-07T23:48:05.029462Z","shell.execute_reply":"2024-04-07T23:48:05.045175Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_jokes_tens = None\nmodels_folder = \"trained_models\"\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx,joke in enumerate(joke_loader):\n        \n        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n        if joke_tens.size()[1] > MAX_SEQ_LEN:\n            continue\n        \n        #The first joke sequence in the sequence\n        if not torch.is_tensor(tmp_jokes_tens):\n            tmp_jokes_tens = joke_tens\n            continue\n        else:\n            #The next joke does not fit in so we process the sequence and leave the last joke \n            #as the start for next sequence \n            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n                work_jokes_tens = tmp_jokes_tens\n                tmp_jokes_tens = joke_tens\n            else:\n                #Add the joke to sequence, continue and try to add more\n                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n                continue\n        ################## Sequence ready, process it trough the model ##################\n            \n        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss = sum_loss + loss.detach().data\n                       \n        proc_seq_count = proc_seq_count + 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 100:\n            print(f\"sum loss {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    # Store the model after each epoch to compare the performance of them\n    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))\n            ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:48:05.046924Z","iopub.execute_input":"2024-04-07T23:48:05.047200Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"EPOCH 0 started==============================\nsum loss 6742.61376953125\nsum loss 6144.7607421875\nsum loss 5619.17626953125\nsum loss 5431.4287109375\nsum loss 5342.4599609375\nsum loss 5281.8984375\nsum loss 5246.8017578125\nsum loss 5216.66845703125\nsum loss 5173.619140625\nEPOCH 1 started==============================\nsum loss 5141.5341796875\nsum loss 5111.5087890625\nsum loss 5076.3466796875\nsum loss 5077.18359375\nsum loss 5046.2998046875\nsum loss 5032.2509765625\nsum loss 5011.74755859375\nsum loss 4988.72412109375\nsum loss 4981.3740234375\nsum loss 4957.92138671875\nEPOCH 2 started==============================\nsum loss 4928.83056640625\nsum loss 4904.892578125\n","output_type":"stream"}]},{"cell_type":"code","source":"MODEL_EPOCH = 4\n\nmodels_folder = \"trained_models\"\n\nmodel_path = os.path.join(models_folder, f\"gpt2_medium_joker_{MODEL_EPOCH}.pt\")\nmodel.load_state_dict(torch.load(model_path))\n\njokes_output_file_path = f'generated_{MODEL_EPOCH}.jokes'\n\nmodel.eval()\nif os.path.exists(jokes_output_file_path):\n    os.remove(jokes_output_file_path)\n    \njoke_num = 0\nwith torch.no_grad():\n   \n        for joke_idx in range(1000):\n        \n            joke_finished = False\n\n            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n\n            for i in range(100):\n                outputs = model(cur_ids, labels=cur_ids)\n                loss, logits = outputs[:2]\n                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n                if i < 3:\n                    n = 20\n                else:\n                    n = 3\n                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n\n                if next_token_id in tokenizer.encode('<|endoftext|>'):\n                    joke_finished = True\n                    break\n\n            \n            if joke_finished:\n                \n                joke_num = joke_num + 1\n                \n                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n                output_text = tokenizer.decode(output_list)\n\n                with open(jokes_output_file_path, 'a') as f:\n                    f.write(f\"{output_text} \\n\\n\")\n                    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}