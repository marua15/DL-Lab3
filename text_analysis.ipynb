{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://alkhalilarabic.com/\n",
      "Text:\n",
      "الخليل لتعليم اللغة العربية اكتسب المهارات اللغوية وتعرف على الثقافة العربية لتفتح عالمك مع منصة الخليل. تعلم العربية في دورات عن بعد على أيدي خبراء انطلق في تعلم العربية وصقل مهاراتك اللغوية في القراءة والكتابة والمحادثة والاستماع من خلال الدورات الأساسية على منصة الخليل تعلم اللغة العربية من الخبراء في اللغة الناطقين بالعربية، وانغمس في ثقافتها، وتعرف على قيمها وحضارتها. تعلم اللغة العربية باحترافية وطوّر مهاراتك اللغوية عبر منصتنا التعليمية المتطورة وأنت في أي موقع من العالم. بإمكانك ممارسة وتطوير مهارة المحادثة لساعات إضافية مع أفضل المدرسين العرب، وفي الأوقات التي تناسبك. اكتسب مهارات لغوية جديدة ترتبط بمجال عملك أو دراستك أو اهتمامك اللغوي في المجالات الدبلوماسية أو الإعلامية أو الأكاديمية أو غيرها أحد مشاريع شركة مؤتلف (إبانة)\n",
      "Score: 0.743\n",
      "\n",
      "URL: https://guidetoarabic.net/ar\n",
      "Text:\n",
      "اللغة العربية هي لغة القرآن الكريم، وهي وسيلة لفهم النصوص الشرعية، والاستنباط الصحيح من النصوص، وتعلم اللغة العربية واجب على المسلمين لفهم القرآن الكريم والسنة النبوية، ولا بد من دراسة العلوم المرتبطة باللغة العربية مثل: علم النحو والبلاغة، والقواعد والإعراب، وعلم البيان والأدب. هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها. مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها. نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم، والتواصل معها مباشرة، من خلال الخارطة الرقمية التي تظهر في أعلى القسم.    هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها.  مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها.     نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم،   دليل إلكترونيّ جامع لخدمات تعليم العربية للناطقين بغيرها، وما يتعلق بها من مبادرات ومشاريع وأفكار، يقدم للعاملين في هذا الحقل ما يعينهم على ترشيد أعمالهم، والوقوف على مواطن الاحتياج والنقص،    الدليل إلى العربية أحد مبادرات مركز أصول\n",
      "Score: 1.566\n",
      "\n",
      "['الخليل لتعليم اللغة العربية اكتسب المهارات اللغوية وتعرف على الثقافة العربية لتفتح عالمك مع منصة الخليل. تعلم العربية في دورات عن بعد على أيدي خبراء انطلق في تعلم العربية وصقل مهاراتك اللغوية في القراءة والكتابة والمحادثة والاستماع من خلال الدورات الأساسية على منصة الخليل تعلم اللغة العربية من الخبراء في اللغة الناطقين بالعربية، وانغمس في ثقافتها، وتعرف على قيمها وحضارتها. تعلم اللغة العربية باحترافية وطوّر مهاراتك اللغوية عبر منصتنا التعليمية المتطورة وأنت في أي موقع من العالم. بإمكانك ممارسة وتطوير مهارة المحادثة لساعات إضافية مع أفضل المدرسين العرب، وفي الأوقات التي تناسبك. اكتسب مهارات لغوية جديدة ترتبط بمجال عملك أو دراستك أو اهتمامك اللغوي في المجالات الدبلوماسية أو الإعلامية أو الأكاديمية أو غيرها أحد مشاريع شركة مؤتلف (إبانة)', 'اللغة العربية هي لغة القرآن الكريم، وهي وسيلة لفهم النصوص الشرعية، والاستنباط الصحيح من النصوص، وتعلم اللغة العربية واجب على المسلمين لفهم القرآن الكريم والسنة النبوية، ولا بد من دراسة العلوم المرتبطة باللغة العربية مثل: علم النحو والبلاغة، والقواعد والإعراب، وعلم البيان والأدب. هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها. مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها. نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم، والتواصل معها مباشرة، من خلال الخارطة الرقمية التي تظهر في أعلى القسم.    هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها.  مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها.     نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم،   دليل إلكترونيّ جامع لخدمات تعليم العربية للناطقين بغيرها، وما يتعلق بها من مبادرات ومشاريع وأفكار، يقدم للعاملين في هذا الحقل ما يعينهم على ترشيد أعمالهم، والوقوف على مواطن الاحتياج والنقص،    الدليل إلى العربية أحد مبادرات مركز أصول']\n",
      "[0.743, 1.566]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape text from a website\n",
    "def scrape_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Modify this based on the structure of the webpage to extract relevant text\n",
    "    text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "    return text\n",
    "\n",
    "# Define URLs of Arabic websites related to your topic\n",
    "urls = [\n",
    "    'https://alkhalilarabic.com/',\n",
    "    'https://guidetoarabic.net/ar'\n",
    "]\n",
    "\n",
    "# Define a function to calculate text score based on some criteria\n",
    "def calculate_text_score(text):\n",
    "    return len(text) / 1000  # Just a simple scaling factor for demonstration\n",
    "\n",
    "# Create a dictionary to store text and corresponding scores\n",
    "data = {}\n",
    "texts =[]\n",
    "scores =[]\n",
    "\n",
    "# Scrape text from each URL and calculate the score\n",
    "for url in urls:\n",
    "    text = scrape_text(url)\n",
    "    texts.append(text)\n",
    "    score = calculate_text_score(text)\n",
    "    scores.append(score)\n",
    "    data[url] = {'text': text, 'score': score}\n",
    "\n",
    "# Print the data\n",
    "for url, info in data.items():\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Text:\\n{info['text']}\")\n",
    "    print(f\"Score: {info['score']}\\n\")\n",
    "    \n",
    "print(texts)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text 1: ['خلل', 'لغة', 'عرب', 'كسب', 'هار', 'لغي', 'عرف', 'ثقف', 'عرب', 'فتح', 'نصة', 'خليل.', 'عرب', 'دور', 'ايد', 'خبراء', 'طلق', 'عرب', 'صقل', 'هرا', 'لغي', 'قرء', 'كتب', 'حدث', 'ماع', 'خلل', 'دور', 'سسي', 'نصة', 'خلل', 'لغة', 'عرب', 'خبراء', 'لغة', 'نطق', 'عربية،', 'غمس', 'ثقافتها،', 'عرف', 'قيم', 'وحضارتها.', 'لغة', 'عرب', 'باحترافية', 'وطر', 'هرا', 'لغي', 'عبر', 'نصت', 'تطر', 'وأن', 'اي', 'وقع', 'عالم.', 'بإم', 'مرس', 'طور', 'هار', 'حدث', 'لسع', 'ضفي', 'فضل', 'درس', 'عرب،', 'وفي', 'اوق', 'بك.', 'كسب', 'هار', 'لغي', 'جدد', 'ربط', 'جال', 'عمل', 'او', 'درس', 'او', 'همم', 'لغي', 'جال', 'دبلوماسية', 'او', 'او', 'اكاديمية', 'او', 'احد', 'مشاريع', 'شرك', 'ؤلف', '(', 'بنة', ')']\n",
      "Preprocessed Text 2: ['لغة', 'عرب', 'لغة', 'قرآ', 'كريم،', 'وهي', 'وسل', 'فهم', 'نصص', 'شرعية،', 'نبط', 'صحح', 'صوص،', 'لغة', 'عرب', 'وجب', 'سلم', 'فهم', 'قرآ', 'كرم', 'سنة', 'ية،', 'بد', 'درس', 'ربط', 'لغة', 'عرب', ':', 'غة،', 'قعد', 'عرب،', 'دب.', 'حدث', 'فضل', 'عرب', 'ومك', 'لغت', 'دلل', 'عجز،', 'وعن', 'زلت', 'سلم', 'وعن', 'سبب', 'دعو', 'وتعليمها.', 'واد', 'تنع', 'بحث', 'لغة', 'عربية،', 'طرق', 'ثلى', 'لتعليمها،', 'نهج', 'تبع', 'جل،', 'اضء', 'شكل', 'لغي', 'طرق', 'معالجتها.', 'وفر', 'قسم', 'قعد', 'كبر', 'تحي', 'عنو', 'جمع', 'كاديم', 'عهد', 'ركز', 'لغة', 'عرب', 'عالم،', 'يتم', 'جمع', 'بحث', 'قرب', 'ركز', 'هم،', 'وصل', 'عها', 'مباشرة،', 'خلل', 'خرط', 'رقم', 'ظهر', 'قسم.', 'حدث', 'فضل', 'عرب', 'ومك', 'لغت', 'دلل', 'عجز،', 'وعن', 'زلت', 'سلم', 'وعن', 'سبب', 'دعو', 'وتعليمها.', 'واد', 'تنع', 'بحث', 'لغة', 'عربية،', 'طرق', 'ثلى', 'لتعليمها،', 'نهج', 'تبع', 'جل،', 'اضء', 'شكل', 'لغي', 'طرق', 'معالجتها.', 'وفر', 'قسم', 'قعد', 'كبر', 'تحي', 'عنو', 'جمع', 'كاديم', 'عهد', 'ركز', 'لغة', 'عرب', 'عالم،', 'يتم', 'جمع', 'بحث', 'قرب', 'ركز', 'هم،', 'دلل', 'كترو', 'جمع', 'خدم', 'عرب', 'نطق', 'غيرها،', 'بدر', 'مشاريع', 'أفكار،', 'قدم', 'عمل', 'حقل', 'يعن', 'رشد', 'اعمالهم،', 'وقف', 'وطن', 'حيج', 'قص،', 'دلل', 'الى', 'عرب', 'احد', 'بدر', 'ركز', 'اصل']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mabrchaouen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\mabrchaouen\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Download stopwords for Arabic\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function for Arabic text preprocessing\n",
    "def preprocess_arabic_text(text):\n",
    "    # Tokenization\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Stemming (Arabic)\n",
    "    stemmer = ISRIStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Stop words removal (Arabic)\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    filtered_tokens = [token for token in stemmed_tokens if token not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Preprocess the Arabic texts\n",
    "preprocessed_texts = [preprocess_arabic_text(text) for text in texts]\n",
    "\n",
    "# Print preprocessed texts\n",
    "for idx, text in enumerate(preprocessed_texts):\n",
    "    print(f\"Preprocessed Text {idx + 1}:\", text)\n",
    "\n",
    "# Convert preprocessed texts back to strings\n",
    "preprocessed_texts = [' '.join(tokens) for tokens in preprocessed_texts]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_texts)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_texts)\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "max_len = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Converting scores to numpy array\n",
    "labels = np.array(scores)\n",
    "\n",
    "# Discretization (if needed)\n",
    "# Assuming you want to convert scores to three discrete categories\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "discretized_labels = discretizer.fit_transform(labels.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0584 - mae: 0.2416 - val_loss: 1.1376 - val_mae: 1.0666\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0547 - mae: 0.2340 - val_loss: 1.1312 - val_mae: 1.0636\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0511 - mae: 0.2261 - val_loss: 1.1246 - val_mae: 1.0605\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0475 - mae: 0.2180 - val_loss: 1.1177 - val_mae: 1.0572\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0438 - mae: 0.2092 - val_loss: 1.1103 - val_mae: 1.0537\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0398 - mae: 0.1995 - val_loss: 1.1022 - val_mae: 1.0498\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0356 - mae: 0.1887 - val_loss: 1.0930 - val_mae: 1.0455\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0311 - mae: 0.1764 - val_loss: 1.0825 - val_mae: 1.0404\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0263 - mae: 0.1621 - val_loss: 1.0702 - val_mae: 1.0345\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0211 - mae: 0.1452 - val_loss: 1.0556 - val_mae: 1.0274\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0609 - mae: 0.2468 - val_loss: 1.1268 - val_mae: 1.0615\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0527 - mae: 0.2295 - val_loss: 1.1181 - val_mae: 1.0574\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0451 - mae: 0.2123 - val_loss: 1.1088 - val_mae: 1.0530\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0377 - mae: 0.1942 - val_loss: 1.0987 - val_mae: 1.0482\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0306 - mae: 0.1749 - val_loss: 1.0875 - val_mae: 1.0429\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0236 - mae: 0.1535 - val_loss: 1.0748 - val_mae: 1.0367\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0167 - mae: 0.1292 - val_loss: 1.0599 - val_mae: 1.0295\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0103 - mae: 0.1013 - val_loss: 1.0424 - val_mae: 1.0210\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0047 - mae: 0.0686 - val_loss: 1.0215 - val_mae: 1.0107\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 9.3447e-04 - mae: 0.0306 - val_loss: 0.9969 - val_mae: 0.9985\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0594 - mae: 0.2437 - val_loss: 1.1107 - val_mae: 1.0539\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0549 - mae: 0.2342 - val_loss: 1.1024 - val_mae: 1.0499\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0505 - mae: 0.2247 - val_loss: 1.0936 - val_mae: 1.0458\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0462 - mae: 0.2149 - val_loss: 1.0843 - val_mae: 1.0413\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0419 - mae: 0.2047 - val_loss: 1.0744 - val_mae: 1.0365\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0376 - mae: 0.1940 - val_loss: 1.0638 - val_mae: 1.0314\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0334 - mae: 0.1826 - val_loss: 1.0523 - val_mae: 1.0258\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0291 - mae: 0.1705 - val_loss: 1.0398 - val_mae: 1.0197\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0248 - mae: 0.1575 - val_loss: 1.0264 - val_mae: 1.0131\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0206 - mae: 0.1434 - val_loss: 1.0117 - val_mae: 1.0058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x291d107d290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNN Model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "rnn_model.add(LSTM(64))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Bidirectional RNN Model\n",
    "bidirectional_rnn_model = Sequential()\n",
    "bidirectional_rnn_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "bidirectional_rnn_model.add(Bidirectional(LSTM(64)))\n",
    "bidirectional_rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "bidirectional_rnn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "bidirectional_rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# GRU Model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "gru_model.add(GRU(64))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0556 - mae: 1.0274\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9969 - mae: 0.9985\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0117 - mae: 1.0058\n",
      "RNN Model Evaluation:\n",
      "Mean Squared Error: 1.0556286573410034\n",
      "Mean Absolute Error: 1.0274379253387451\n",
      "\n",
      "Bidirectional RNN Model Evaluation:\n",
      "Mean Squared Error: 0.9969407916069031\n",
      "Mean Absolute Error: 0.9984692335128784\n",
      "\n",
      "GRU Model Evaluation:\n",
      "Mean Squared Error: 1.0117295980453491\n",
      "Mean Absolute Error: 1.005847692489624\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "rnn_scores = rnn_model.evaluate(X_test, y_test)\n",
    "bidirectional_rnn_scores = bidirectional_rnn_model.evaluate(X_test, y_test)\n",
    "gru_scores = gru_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"RNN Model Evaluation:\")\n",
    "print(\"Mean Squared Error:\", rnn_scores[0])\n",
    "print(\"Mean Absolute Error:\", rnn_scores[1])\n",
    "\n",
    "print(\"\\nBidirectional RNN Model Evaluation:\")\n",
    "print(\"Mean Squared Error:\", bidirectional_rnn_scores[0])\n",
    "print(\"Mean Absolute Error:\", bidirectional_rnn_scores[1])\n",
    "\n",
    "print(\"\\nGRU Model Evaluation:\")\n",
    "print(\"Mean Squared Error:\", gru_scores[0])\n",
    "print(\"Mean Absolute Error:\", gru_scores[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
